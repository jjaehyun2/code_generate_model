import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer
import time
import os
import psutil
import gc
from pathlib import Path
if os.path.exists("temp_model.pt"):
    os.remove("temp_model.pt") 

os.environ["CUDA_VISIBLE_DEVICES"] = "6" 

class ModelOptimizer:
    def __init__(self, model_path):
        self.model_path = model_path
        self.original_model = None
        self.optimized_model = None
        self.tokenizer = None
        
    def load_model(self):
        """ì›ë³¸ ëª¨ë¸ ë¡œë“œ"""
        print("ì›ë³¸ ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
        self.original_model = AutoModelForCausalLM.from_pretrained(self.model_path, torch_dtype=torch.float32)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        print("âœ“ ì›ë³¸ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    
    def apply_optimization(self):
        if torch.cuda.is_available():
            self.optimized_model = self.original_model.to(torch.bfloat16)
        else:
            self.optimized_model = torch.quantization.quantize_dynamic(
                self.original_model,
                {torch.nn.Linear},
                dtype=torch.qint8
            )
        return self.optimized_model
    
    def get_model_size(self, model):
        """ëª¨ë¸ í¬ê¸° ì¸¡ì • (ì„ì‹œ ì €ì¥ íŒŒì¼ í¬ê¸°)"""
        ts=int(time.time()*1000)
        temp_path = "temp_model_{ts}.pt"
        torch.save(model, temp_path)
        size_mb = os.path.getsize(temp_path) / 1e6
        Path(temp_path).unlink(missing_ok=True)  # íŒŒì¼ ì‚­ì œ
        return size_mb
    
    def benchmark_inference(self, model, test_prompts, num_runs=5):
        """ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ (í‰ê·  ì‹œê°„, ë©”ëª¨ë¦¬ ì‚¬ìš© ì¸¡ì •)"""
        model.eval()
        latencies = []
        memory_usage = []
        
        with torch.no_grad():
            for i in range(num_runs):
                prompt = test_prompts[i % len(test_prompts)]
                
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    start_memory = torch.cuda.memory_allocated() / 1e6
                else:
                    start_memory = psutil.Process().memory_info().rss / 1e6
                
                start_time = time.time()
                
                inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=256)
                
                if model.dtype == torch.bfloat16:
                    inputs = {k: v.to(torch.bfloat16) if v.dtype == torch.float32 else v for k, v in inputs.items()}
                
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=100,
                    do_sample=True,
                    temperature=0.7,
                    pad_token_id=self.tokenizer.eos_token_id
                )
                
                end_time = time.time()
                
                if torch.cuda.is_available():
                    end_memory = torch.cuda.memory_allocated() / 1e6
                else:
                    end_memory = psutil.Process().memory_info().rss / 1e6
                
                latencies.append(end_time - start_time)
                memory_usage.append(end_memory - start_memory)
        
        return {
            'avg_latency': sum(latencies) / len(latencies),
            'avg_memory': sum(memory_usage) / len(memory_usage),
            'min_latency': min(latencies),
            'max_latency': max(latencies)
        }
    
    def compare_models(self, test_prompts):
        """ì›ë³¸ ëª¨ë¸ê³¼ ìµœì í™” ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë° ë¡œê·¸ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë¶„ì„")
        print("="*60)
        
        original_size = self.get_model_size(self.original_model)
        optimized_size = self.get_model_size(self.optimized_model)
        
        print(f"\nğŸ“Š ëª¨ë¸ í¬ê¸° ë¹„êµ:")
        print(f"   ì›ë³¸ ëª¨ë¸: {original_size:.1f} MB")
        print(f"   ìµœì í™” ëª¨ë¸: {optimized_size:.1f} MB")
        print(f"   í¬ê¸° ê°ì†Œ: {((original_size - optimized_size) / original_size * 100):.1f}%")
        
        print(f"\n ì¶”ë¡  ì„±ëŠ¥ ë¹„êµ:")
        print("   ì›ë³¸ ëª¨ë¸ ë²¤ì¹˜ë§ˆí‚¹...")
        original_perf = self.benchmark_inference(self.original_model, test_prompts)
        
        print("   ìµœì í™” ëª¨ë¸ ë²¤ì¹˜ë§ˆí‚¹...")
        optimized_perf = self.benchmark_inference(self.optimized_model, test_prompts)
        
        print(f"\n   ì›ë³¸ ëª¨ë¸:")
        print(f"     í‰ê·  ì¶”ë¡  ì‹œê°„: {original_perf['avg_latency']:.3f}ì´ˆ")
        print(f"     í‰ê·  ë©”ëª¨ë¦¬ ì‚¬ìš©: {original_perf['avg_memory']:.1f}MB")
        
        print(f"\n   ìµœì í™” ëª¨ë¸:")
        print(f"     í‰ê·  ì¶”ë¡  ì‹œê°„: {optimized_perf['avg_latency']:.3f}ì´ˆ")
        print(f"     í‰ê·  ë©”ëª¨ë¦¬ ì‚¬ìš©: {optimized_perf['avg_memory']:.1f}MB")
        
        speed_improvement = (original_perf['avg_latency'] - optimized_perf['avg_latency']) / original_perf['avg_latency'] * 100
        if original_perf['avg_memory'] > 0:
            memory_reduction = (original_perf['avg_memory'] - optimized_perf['avg_memory']) / original_perf['avg_memory'] * 100
        else:
            memory_reduction = 0
        
        print(f"\n ì„±ëŠ¥ ê°œì„  íš¨ê³¼:")
        print(f"   ì¶”ë¡  ì†ë„ ê°œì„ : {speed_improvement:.1f}%")
        print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ: {memory_reduction:.1f}%")
        
        return {
            'size_reduction': ((original_size - optimized_size) / original_size * 100),
            'speed_improvement': speed_improvement,
            'memory_reduction': memory_reduction
        }
    
    def save_optimized_model(self, save_dir: str="optimized_model"):
        Path(save_dir).mkdir(parents=True, exist_ok=True)
        self.optimized_model.save_pretrained(save_dir)
        self.tokenizer.save_pretrained(save_dir)
        print(f"ìµœì í™”ëœ ëª¨ë¸ì„ {save_dir}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    
    def load_optimized_model(self, load_dir: str="optimized_model"):
        model = AutoModelForCausalLM.from_pretrained(load_dir)
        tokenizer = AutoTokenizer.from_pretrained(load_dir)
        return model, tokenizer

def main():
    model_path = "./finetuned_model/finetuned_V1_quantized_pruned"  
    optimizer = ModelOptimizer(model_path)
    optimizer.load_model()
    

    optimizer.apply_optimization()
    
    test_prompts = [
        "Generate a Python function to calculate fibonacci numbers:",
        "Create a function to sort a list of integers:",
        "Write a Python class for a binary tree:",
        "Implement a function to check if a string is palindrome:",
        "Generate code for reading a CSV file:"
    ]
    
    results = optimizer.compare_models(test_prompts)
    
    optimizer.save_optimized_model("finetuned_model/finetuned_V2_optimized")
    
    print(f"\nìµœì¢… ìµœì í™” ê²°ê³¼:")
    print(f" ëª¨ë¸ í¬ê¸°: {results['size_reduction']:.1f}% ê°ì†Œ")
    print(f" ì¶”ë¡  ì†ë„: {results['speed_improvement']:.1f}% í–¥ìƒ")
    print(f" ë©”ëª¨ë¦¬ ì‚¬ìš©: {results['memory_reduction']:.1f}% ì ˆì•½")

if __name__ == "__main__":
    main()
